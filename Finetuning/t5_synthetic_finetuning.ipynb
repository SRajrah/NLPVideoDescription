{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c760ed1-060e-40bc-b235-4bebb3da6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "#pip install scikit-learn\n",
    "#pip install torch\n",
    "#pip install numpy\n",
    "#pip install pandas\n",
    "#pip install transformers\n",
    "#pip install torch torchvision\n",
    "#pip install SentencePiece\n",
    "#pip install ipywidgets\n",
    "#pip install --upgrade pip setuptools wheel\n",
    "#pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba36f4a9-8775-4faa-81c4-1f21d9771f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tdqm in ./.local/lib/python3.12/site-packages (0.0.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.12/site-packages (from tdqm) (4.67.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75158c9a-129b-41c8-a460-7c8e571a9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11aedb1f-00e8-4b91-90c8-883919a04fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/home1/kmayya/Pipeline/synthetic_dataset.csv\"\n",
    "df = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef24c35-eeba-4e46-9953-5253db423e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df['dense_frame_descriptions']\n",
    "y = df['description']\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1,  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759cd773-1d0a-4250-96e1-60fd53379f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd2b8b3-d79c-4e01-b1af-6085471989f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.dense_frame_descriptions\n",
    "        self.ctext = self.data.description\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc100db8-4849-4cdd-9988-2ea855bfb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(loader, 0)):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807d504d-a3a5-4260-ab16-a9e5e3f9efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0c384ae-7845-41ed-a62d-8bab1fb2ce42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lkdevftq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-valley-18</strong> at: <a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization/runs/lkdevftq' target=\"_blank\">https://wandb.ai/kmayya-usc/transformers_tutorials_summarization/runs/lkdevftq</a><br/> View project at: <a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization' target=\"_blank\">https://wandb.ai/kmayya-usc/transformers_tutorials_summarization</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241213_104824-lkdevftq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lkdevftq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home1/kmayya/wandb/run-20241213_104848-7rnaz5ow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization/runs/7rnaz5ow' target=\"_blank\">firm-firefly-19</a></strong> to <a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization' target=\"_blank\">https://wandb.ai/kmayya-usc/transformers_tutorials_summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kmayya-usc/transformers_tutorials_summarization/runs/7rnaz5ow' target=\"_blank\">https://wandb.ai/kmayya-usc/transformers_tutorials_summarization/runs/7rnaz5ow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            dense_frame_descriptions  \\\n",
      "0  summarize keyframe captions: ['a large lawn in...   \n",
      "1  summarize keyframe captions: ['a rodeo arena w...   \n",
      "2  summarize keyframe captions: ['a group of youn...   \n",
      "3  summarize keyframe captions: [\"a close-up of a...   \n",
      "4  summarize keyframe captions: [\"a person's hand...   \n",
      "\n",
      "                                         description  \n",
      "0  A backyard is shown as a man sitting on an ele...  \n",
      "1  A bull comes out of the gate at a rodeo and  m...  \n",
      "2  We see people doing a dance routine in a baske...  \n",
      "3  A man is standing outside in his front lawn wi...  \n",
      "4  A towel and glass are on a table next to a pie...  \n",
      "FULL Dataset: (1027, 2)\n",
      "TRAIN Dataset: (822, 2)\n",
      "TEST Dataset: (205, 2)\n",
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home1/kmayya/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  11.284489631652832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "411it [00:56,  7.26it/s]\n",
      "2it [00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  1.495384931564331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "411it [00:53,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
      "Completed 0\n",
      "Completed 100\n",
      "Output Files generated for review\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "\n",
    "    config = wandb.config        \n",
    "    config.TRAIN_BATCH_SIZE = 2    \n",
    "    config.VALID_BATCH_SIZE = 2    \n",
    "    config.TRAIN_EPOCHS = 2       \n",
    "    config.VAL_EPOCHS = 1 \n",
    "    config.LEARNING_RATE = 1e-4    \n",
    "    config.SEED = 42              \n",
    "    config.MAX_LEN = 512\n",
    "    config.SUMMARY_LEN = 150 \n",
    "\n",
    "    torch.manual_seed(config.SEED) \n",
    "    np.random.seed(config.SEED) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "    df = pd.read_csv(train_file)\n",
    "    df = df[['dense_frame_descriptions','description']]\n",
    "    df.dense_frame_descriptions = 'summarize keyframe captions: ' + df.dense_frame_descriptions\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "    train_params = {\n",
    "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': config.VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(config.TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), 't5_on_synthetic.pth')\n",
    "\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config.VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa48e334-eec1-42df-8457-d21dfac35339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_28397548/ipykernel_2909/1042409953.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/home1/kmayya/Pipeline/t5_on_synthetic.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: keyframe captions: \"a man in a kitchen cutting through a piece of wood. He is wearing a gray shirt and appears to be in the process of cutting through a piece of wood. He is holding a chainsaw and appears to be in the process of cutting through a piece of wood. The kitchen has white cabinets and a tiled floor.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"summarize keyframe captions: a man in a kitchen, standing in front of a white countertop with a computer monitor on it. He is holding a chainsaw and appears to be in the process of cutting through a piece of wood. The kitchen has white cabinets and a tiled floor. There is a sink and a window in the background, and a blue curtain on the right side of the image. The man is wearing a gray shirt and is focused on the task at hand.\", return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home1/kmayya/Pipeline/t5_on_synthetic.pth\"))\n",
    "model.eval()\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=1500,  \n",
    "    num_beams=5,    \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Output:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118ddab-709f-41f8-b00e-5060f48d5821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
